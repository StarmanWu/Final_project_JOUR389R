---
title: "Final_Summary"
author: "Xingman Wu"
date: "2024-11-12"
output: html_document
---
### Load the appropriate software libraries
```{r}
library(tidyverse)
#library(pdftools)
#install.packages("striprtf")
library(striprtf)
install.packages("unrtf")
library(unrtf)
library("readxl")


### load the title xlsx
AItitle <- read_excel("../final_summary/assets/PDF/AIhealth.XLSX")
head(AItitle)

### load the RTF doc and folder
test <- unrtf("../final_summary/assets/PDF/Files.RTF", format = "text")
writeLines(test, "../final_summary/assets/extracted_text/AIhealth.txt")



#Using pdftools package. Good for basic PDF extraction
#text <- pdf_text("../final_summary/assets/PDF/Files.RTF") 
#pdf_text reads the text from a PDF file.
#writeLines(text, "../final_summary/assets/extracted_text/AIhealth.txt")
#writeLines writes this text to a text file


```
### Load the data
```{r}
# Step 1: Read the entire text file into R
#For Mac: In Finder, Cntl + click on the filename, NOW hold down Alt/Option, and an item to copy file path will appear as Copy "Filename" as Pathname
file_path <- "../final_summary/assets/extracted_text/AIhealth.txt"
text_data <- readLines(file_path)

# Step 2: Combine lines into one single string
text_combined <- paste(text_data, collapse = "\n")

# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "Load-Date")[[1]]

# Step 4: Write each section to a new file
output_dir <- "../final_summary/assets/extracted_text/"
for (i in seq_along(documents)) {
  output_file <- file.path(output_dir, paste0("AI_extracted_", i, ".txt"))
  writeLines(documents[[i]], output_file)
}

cat("Files created:", length(documents), "\n")

```


### Delete the nonuseful lines in extracted.txts, only keep the content of the news (but this is not a perfect method, some txts lost their content, I will keep exploring)
```{r}
# Load required library
library(stringr)

# Function to process a single file
process_file <- function(input_path, output_path) {
  # Read the content of the file
  file_content <- readLines(input_path, warn = FALSE)
  
  # Combine lines into a single text
  full_text <- paste(file_content, collapse = "\n")
  
  # Find the last occurrence of ".jpg"
  last_jpg_pos <- str_locate_all(full_text, "\\.jpg")[[1]]
  if (length(last_jpg_pos) > 0) {
    # Get the position of the last ".jpg"
    last_jpg_end <- last_jpg_pos[nrow(last_jpg_pos), 2]
    
    # Keep the content after the last ".jpg"
    modified_content <- substr(full_text, last_jpg_end + 1, nchar(full_text))
  } else {
    # If no ".jpg" is found, keep the content as is
    modified_content <- full_text
  }
  
  # Write the modified content back to a new file
  writeLines(modified_content, output_path)
}

# Directory paths
input_dir <- "../final_summary/assets/extracted_text"  # Replace with the path to your input files
output_dir <- "../final_summary/assets/extracted_text"  # Replace with the path for output files

# Get a list of .txt files in the input directory
txt_files <- list.files(input_dir, pattern = "\\.txt$", full.names = TRUE)

# Process each file
for (file_path in txt_files) {
  # Construct the output file path
  output_path <- file.path(output_dir, basename(file_path))
  
  # Process the file
  process_file(file_path, output_path)
}

cat("Processing completed. Modified files are saved in", output_dir, "\n")

```


#Build a final dataframe index. Mutate the content of the news as a new column
```{r}
#This creates an index with the file path to the stories. And then it compiles the stories into a dataframe
#####################
# Begin SM Code #####
#####################

###
# List out text files that match pattern .txt, create DF
###

files <- list.files("../final_summary/assets/extracted_text", pattern="*.txt") %>% 
  as.data.frame() |> 
  rename(filename = 1) |> 
  #create an index with the file name
 mutate(index = str_extract(filename, "\\d+")) |> 
  mutate(index = as.numeric(index))
View(files)

#Join the file list to the index

#load final data if you haven't already
#final_data <- read.csv("assets/final_data.csv")

AItitle <- AItitle %>% mutate(index = row_number())
final_index <- AItitle |> 
  inner_join(files, c("index")) |> 
#you need the actual hard-coded path on this line below to the text
  mutate(filepath = paste0("../exercises/assets/extracted_text/", filename))

head(final_index)
View(final_index) # this is the dataset I will use
```


#Include a link to the source data that you have compiled in your GitHub repository
```{r}

```

#Spell out your content analysis plan, including preliminary work on a code book

1. Topic: What are people talking about? - word frequency/ co-occurrence analysis. (theme - code book; topic modeling result)

2. Perception and concerns: How do people perceive it (positive or negative)? Why? What are they happy about or worried about? - sentimental analysis & word frequency. 

3. Timeline: How do the aforementioned issues evolve as more models emerge? - articles will be analyzed in groups of specific periods (e,g., GPT-1, June 2018, Introduced transformer-based approach for pre-training; GPT-3, June 2020, Significantly larger scale, advanced capabilities in NLP tasks; GPT-4, March 14, 2023, Applied GPT-4's capabilities to conversational AI; ).

Preliminary work on a code book: Advanced tech introduction; Application in medicine, healthcare, and clinical settings; Industry forecast.
 


#Provide a sample of the data and some descriptive statistics.
```{r}
final_index$pb <- final_index$`Published date`

final_index <- final_index |> 
  janitor::clean_names() |> 
  mutate(
    date2 =  mdy(pb),
    year = year(date2),
    month = month(date2))


#Using code, describe the number of rows and columns of the dataset
dim(final_index) # 172 rows and 28 columns

#Create ggplot chart showing the distribution of the data over time
Months <- final_index %>% 
  count(month) %>% 
  group_by(month) %>% 
  arrange(desc(month))
head(Months)

ggplot(Months,aes(x = month, y = n,
             fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
#This is your title sequence
  labs(title = "AI-health related news articles By Month, US",
       subtitle = "2024-01-01 to 2024-11-18",
       caption = "Graphic by Xingman Wu, 11-20-2024",
       y="Number of Articles",
       x="Month")
```


